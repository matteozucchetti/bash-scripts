#!/bin/bash

prefixes="
... list of domain prefixes here
"

urllist="
... list of slugs here
"

## find and remove duplicates
urllist=$(echo $urllist | tr ' ' '\n' | sort -u | tr '\n' ' ')

## Loop through the list of prefixes and append each URL to the list, writing each set of URLs to a different file (24 different files)
for prefix in $prefixes
do
    ## remove the protocol from the prefix
    name=$(echo $prefix | sed 's/https:\/\///g')
    name=$(echo $name | sed 's/www.thenorthface.//g')
    ## remove the slashes from the prefix
    name=$(echo $name | sed 's/\///g')
    echo $name
    touch "urls/$name.txt"
    ## write each set of URLs to a different file
    for url in $urllist
    do
        echo "$prefix$url" >> "urls/$name.txt"
    done
done

# loop through all the files and all the urls in each file and run the curl command, appending the status output next to the URL ", 200" or ", 404" (writing to the respective prefix file)
for file in urls/*.txt
do
    while read url
    do
        status=$(curl -s -o /dev/null -w "%{http_code}" $url)
        echo "$url, $status" >> "output/$(basename $file)"
    done < $file
done